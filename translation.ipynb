{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "from torch import nn\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "num_steps = 35\n",
    "train_iter, source_Vocab, target_Vocab = utils.get_train_iter(batch_size, num_steps)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.LSTM(embed_size, num_hiddens, num_layers, bidirectional=bidirectional)\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.bidirectional = bidirectional\n",
    "        if bidirectional:\n",
    "            # 由于每一层有两个方向,因此需要将两个方向进行合并\n",
    "            self.linear_hidden = nn.Linear(self.num_hiddens * 2, self.num_hiddens)\n",
    "            self.linear_content = nn.Linear(self.num_hiddens * 2, self.num_hiddens)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.embedding(X)\n",
    "        X = X.permute(1, 0, 2)\n",
    "        output, state = self.rnn(X)\n",
    "        hidden_state, content_state = state\n",
    "        if self.bidirectional:\n",
    "            # 将每一层的正反state拼在一起,再放入神经网络中,使得与decoder的num_hiddens一致\n",
    "            hidden_state = torch.cat(\n",
    "                [hidden_state[:self.num_layers * 2:2, :, :], hidden_state[1:self.num_layers * 2 + 1:2, :, :]], dim=2)\n",
    "            content_state = torch.cat(\n",
    "                [content_state[:self.num_layers * 2:2, :, :], content_state[1:self.num_layers * 2 + 1:2, :, :]], dim=2)\n",
    "            hidden_state = self.linear_hidden(hidden_state)\n",
    "            content_state = self.linear_content(content_state)\n",
    "        return hidden_state, content_state\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.LSTM(embed_size + num_hiddens * 2, num_hiddens, num_layers)\n",
    "        self.linear = nn.Linear(num_hiddens, vocab_size)\n",
    "\n",
    "    def init_state(self, encoder_output_state):\n",
    "        return encoder_output_state\n",
    "\n",
    "    def forward(self, X, state, predict=False):\n",
    "        if not predict:\n",
    "            X = self.embedding(X).permute(1, 0, 2)\n",
    "            # 由于decoder的信息全由encoder的最后一个时间state得到,\n",
    "            # 因此最后一个state的最后一层很重要,要尽可能的充分利用,\n",
    "            # 因此将最后一个state的最后一层也作为decoder的输入\n",
    "            hidden_state, content_state = state\n",
    "            new_hidden_state = torch.cat([hidden_state[-1].unsqueeze(0)] * X.shape[0], dim=0)\n",
    "            new_content_state = torch.cat([content_state[-1].unsqueeze(0)] * X.shape[0], dim=0)\n",
    "            X = torch.cat([new_hidden_state, new_content_state, X], dim=2)\n",
    "        # X 的shape为:(num_steps, batch_size, decoder_embed_size + encoder_hidden_num * 2)\n",
    "        output, state = self.rnn(X, state)\n",
    "        output = self.linear(output).permute(1, 0, 2)\n",
    "        return output, state\n",
    "\n",
    "\n",
    "class Myloss(nn.CrossEntropyLoss):\n",
    "    def value_mask(self, X, valid_len):\n",
    "        mask = torch.arange(X.shape[1], dtype=torch.float32, device=X.device)[None, :] > valid_len[:, None]\n",
    "        X[mask] = 0\n",
    "        return X\n",
    "\n",
    "    def forward(self, predict, target, valid_len=None):\n",
    "        weights = torch.ones_like(target)\n",
    "        weights = self.value_mask(weights, valid_len)\n",
    "        self.reduction = 'none'\n",
    "        unweighted_loss = super().forward(predict.permute(0, 2, 1), target)\n",
    "        weighted_loss = unweighted_loss * weights\n",
    "        return weighted_loss.mean()\n",
    "\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        encoder_output_state = self.encoder(source)\n",
    "        decoder_init_state = self.decoder.init_state(encoder_output_state)\n",
    "        return self.decoder(target, decoder_init_state)\n",
    "def predict(net, source_sentence, source_Vocab, target_Vocab, num_steps, device):\n",
    "    # 用于存储译文\n",
    "    result = []\n",
    "    # 原文\n",
    "    source = source_Vocab.prase(source_sentence).to(device)\n",
    "    # 获取最后一个状态\n",
    "    state = net.encoder(source)\n",
    "    # 获取encoder的最后一个state的信息\n",
    "    hidden_state, content_state = state\n",
    "    new_hidden_state = hidden_state[-1].unsqueeze(0)\n",
    "    new_content_state = content_state[-1].unsqueeze(0)\n",
    "    # 初始化decoder的第一个状态\n",
    "    state = net.decoder.init_state(state)\n",
    "    # 构造翻译的第一个词\n",
    "    X = torch.tensor(target_Vocab.word_to_index['<eos>']).reshape(-1, 1).to(device)\n",
    "    X = net.decoder.embedding(X).permute(1, 0, 2)\n",
    "    X = torch.cat([new_hidden_state, new_content_state, X], dim=2)\n",
    "    for i in range(num_steps):\n",
    "        # 开启预测模式,进行预测\n",
    "        Y, state = net.decoder(X, state, True)\n",
    "        X = Y.argmax(dim=2)\n",
    "        # 获取最大概率的index\n",
    "        pred = X.squeeze(dim=0).type(torch.int32).item()\n",
    "        # 如果ind ex为eos,则停止预测\n",
    "        if pred == target_Vocab.word_to_index['<eos>']:\n",
    "            break\n",
    "        X = net.decoder.embedding(X).permute(1, 0, 2)\n",
    "        X = torch.cat([new_hidden_state, new_content_state, X], dim=2)\n",
    "        result.append(pred)\n",
    "    return ' '.join(target_Vocab.to_word(result))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def train(net, data_iter, lr, num_epochs, device,source_Vocab, target_Vocab,num_steps):\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss = Myloss()\n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in data_iter:\n",
    "            optimizer.zero_grad()\n",
    "            # 将数据放到device上\n",
    "            source, source_valid_len, target, target_valid_len = [x.to(device) for x in batch]\n",
    "            # 再每一个句子前面添加<bos>的index,bos的index为2\n",
    "            bos = torch.tensor([2] * target.shape[0], device=device).reshape(-1, 1)\n",
    "            decoder_input = torch.cat([bos, target[:, :-1]], dim=1)\n",
    "            # 进行优化\n",
    "            Y_hat, _ = net(source, decoder_input)\n",
    "            l = loss(Y_hat, target, target_valid_len)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "        print(epoch,l)\n",
    "        print(predict(net, 'Don\\'t you want some ice cream?', source_Vocab, target_Vocab, num_steps, device))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(1.0513, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "1 tensor(1.0205, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "2 tensor(1.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "3 tensor(0.9851, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "4 tensor(0.9733, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "5 tensor(0.9633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "6 tensor(0.9537, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "7 tensor(0.9479, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "8 tensor(0.9396, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "9 tensor(0.9350, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "10 tensor(0.9287, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "11 tensor(0.9258, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "12 tensor(0.9210, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "13 tensor(0.9152, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "14 tensor(0.9132, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "15 tensor(0.9098, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "16 tensor(0.9066, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "17 tensor(0.9018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "18 tensor(0.8990, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "19 tensor(0.8942, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "20 tensor(0.8913, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "21 tensor(0.8892, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "22 tensor(0.8857, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "23 tensor(0.8830, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "24 tensor(0.8799, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "25 tensor(0.8783, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "26 tensor(0.8753, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "27 tensor(0.8734, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "28 tensor(0.8714, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "29 tensor(0.8696, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "30 tensor(0.8671, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "31 tensor(0.8646, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "32 tensor(0.8623, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "33 tensor(0.8611, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "34 tensor(0.8591, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "35 tensor(0.8577, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "36 tensor(0.8547, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "37 tensor(0.8539, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "38 tensor(0.8522, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "39 tensor(0.8518, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "40 tensor(0.8488, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "41 tensor(0.8473, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "42 tensor(0.8448, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "43 tensor(0.8452, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "44 tensor(0.8425, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "45 tensor(0.8424, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "46 tensor(0.8386, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "47 tensor(0.8387, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "48 tensor(0.8366, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "49 tensor(0.8359, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "50 tensor(0.8360, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne veux-tu pas de glace ?\n",
      "51 tensor(0.8348, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "52 tensor(0.8335, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "53 tensor(0.8319, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "54 tensor(0.8308, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "55 tensor(0.8300, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "56 tensor(0.8296, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "57 tensor(0.8261, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "58 tensor(0.8259, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "59 tensor(0.8246, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "60 tensor(0.8232, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "61 tensor(0.8225, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "62 tensor(0.8217, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "63 tensor(0.8209, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "64 tensor(0.8181, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "65 tensor(0.8161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "66 tensor(0.8161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "67 tensor(0.8155, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "68 tensor(0.8157, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "69 tensor(0.8145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "70 tensor(0.8137, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "71 tensor(0.8110, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "72 tensor(0.8106, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "73 tensor(0.8101, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "74 tensor(0.8090, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n",
      "75 tensor(0.8075, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "ne voulez-vous pas de glace ?\n"
     ]
    }
   ],
   "source": [
    "encoder_embed_size = 200\n",
    "decoder_embed_size = 200\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "encoder = Encoder(len(source_Vocab), decoder_embed_size, hidden_size, num_layers, True)\n",
    "decoder = Decoder(len(target_Vocab), decoder_embed_size, hidden_size, num_layers)\n",
    "net = EncoderDecoder(encoder, decoder)\n",
    "net.load_state_dict(torch.load('./model.pth'))\n",
    "num_epoch = 200\n",
    "lr = 0.001\n",
    "device = 'cuda'\n",
    "train(net, train_iter, lr, num_epoch, device,source_Vocab, target_Vocab,num_steps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predict(net, 'Ask Tom where he bought his hat.', source_Vocab, target_Vocab, num_steps, device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# torch.save(net.state_dict(), './translation_withoutAttention/model.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
