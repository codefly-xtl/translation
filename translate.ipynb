{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "from torch import nn\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_steps = 20\n",
    "train_iter, source_Vocab, target_Vocab = utils.get_train_iter(batch_size, num_steps)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.LSTM(embed_size, num_hiddens, num_layers, bidirectional=bidirectional)\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hiddens = num_hiddens\n",
    "        if bidirectional:\n",
    "            # 由于每一层有两个方向,因此需要将两个方向进行合并\n",
    "            self.linear_hidden = nn.Linear(self.num_hiddens * 2, self.num_hiddens)\n",
    "            self.linear_content = nn.Linear(self.num_hiddens * 2, self.num_hiddens)\n",
    "            self.bidirectional = True\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.embedding(X)\n",
    "        X = X.permute(1, 0, 2)\n",
    "        output, state = self.rnn(X)\n",
    "        hidden_state, content_state = state\n",
    "        if self.bidirectional:\n",
    "            # 将每一层的正反state拼在一起,再放入神经网络中,使得与decoder的num_hiddens一致\n",
    "            hidden_state = torch.cat(\n",
    "                [hidden_state[:self.num_layers * 2:2, :, :], hidden_state[1:self.num_layers * 2 + 1:2, :, :]], dim=2)\n",
    "            content_state = torch.cat(\n",
    "                [content_state[:self.num_layers * 2:2, :, :], content_state[1:self.num_layers * 2 + 1:2, :, :]], dim=2)\n",
    "            hidden_state = self.linear_hidden(hidden_state)\n",
    "            content_state = self.linear_content(content_state)\n",
    "        return hidden_state, content_state\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.LSTM(embed_size + num_hiddens * 2, num_hiddens, num_layers)\n",
    "        self.linear = nn.Linear(num_hiddens, vocab_size)\n",
    "\n",
    "    def init_state(self, encoder_output_state):\n",
    "        return encoder_output_state\n",
    "\n",
    "    def forward(self, X, state, predict=False):\n",
    "        if not predict:\n",
    "            X = self.embedding(X).permute(1, 0, 2)\n",
    "            # 由于decoder的信息全由encoder的最后一个时间state得到,\n",
    "            # 因此最后一个state的最后一层很重要,要尽可能的充分利用,\n",
    "            # 因此将最后一个state的最后一层也作为decoder的输入\n",
    "            hidden_state, content_state = state\n",
    "            new_hidden_state = torch.cat([hidden_state[-1].unsqueeze(0)] * X.shape[0], dim=0)\n",
    "            new_content_state = torch.cat([content_state[-1].unsqueeze(0)] * X.shape[0], dim=0)\n",
    "            X = torch.cat([new_hidden_state, new_content_state, X], dim=2)\n",
    "        # X 的shape为:(num_steps, batch_size, decoder_embed_size + encoder_hidden_num * 2)\n",
    "        output, state = self.rnn(X, state)\n",
    "        output = self.linear(output).permute(1, 0, 2)\n",
    "        return output, state\n",
    "\n",
    "\n",
    "def value_mask(X, valid_len):\n",
    "    mask = torch.arange(X.shape[1], dtype=torch.float32, device=X.device)[None, :] > valid_len[:, None]\n",
    "    X[mask] = 0\n",
    "    return X\n",
    "\n",
    "\n",
    "class Myloss(nn.CrossEntropyLoss):\n",
    "    def forward(self, predict, target, valid_len=None):\n",
    "        weights = torch.ones_like(target)\n",
    "        weights = value_mask(weights, valid_len)\n",
    "        self.reduction = 'none'\n",
    "        unweighted_loss = super().forward(predict.permute(0, 2, 1), target)\n",
    "        weighted_loss = unweighted_loss * weights\n",
    "        return weighted_loss.mean()\n",
    "\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        encoder_output_state = self.encoder(source)\n",
    "        decoder_init_state = self.decoder.init_state(encoder_output_state)\n",
    "        return self.decoder(target, decoder_init_state)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def train(net, data_iter, lr, num_epochs, device):\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss = Myloss()\n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in data_iter:\n",
    "            optimizer.zero_grad()\n",
    "            # 将数据放到device上\n",
    "            source, source_valid_len, target, target_valid_len = [x.to(device) for x in batch]\n",
    "            # 再每一个句子前面添加<bos>的index,bos的index为2\n",
    "            bos = torch.tensor([2] * target.shape[0], device=device).reshape(-1, 1)\n",
    "            decoder_input = torch.cat([bos, target[:, :-1]], dim=1)\n",
    "            # 进行优化\n",
    "            Y_hat, _ = net(source, decoder_input)\n",
    "            l = loss(Y_hat, target, target_valid_len)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "    print('训练完毕', l)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "encoder_embed_size = 300\n",
    "decoder_embed_size = 300\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "encoder = Encoder(len(source_Vocab), decoder_embed_size, hidden_size, num_layers, True)\n",
    "decoder = Decoder(len(target_Vocab), decoder_embed_size, hidden_size, num_layers)\n",
    "net = EncoderDecoder(encoder, decoder)\n",
    "num_epoch = 200\n",
    "lr = 0.005\n",
    "device = 'cuda'\n",
    "train(net, train_iter, lr, num_epoch, device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict(net, source_sentence, source_Vocab, target_Vocab, num_steps, device):\n",
    "    # 用于存储译文\n",
    "    result = []\n",
    "    # 原文\n",
    "    source = source_Vocab.prase(source_sentence).to(device)\n",
    "    # 获取最后一个状态\n",
    "    state = net.encoder(source)\n",
    "    # 获取encoder的最后一个state的信息\n",
    "    hidden_state, content_state = state\n",
    "    new_hidden_state = hidden_state[-1].unsqueeze(0)\n",
    "    new_content_state = content_state[-1].unsqueeze(0)\n",
    "    # 初始化decoder的第一个状态\n",
    "    state = net.decoder.init_state(state)\n",
    "    # 构造翻译的第一个词\n",
    "    X = torch.tensor(target_Vocab.word_to_index['<eos>']).reshape(-1, 1).to(device)\n",
    "    X = net.decoder.embedding(X).permute(1, 0, 2)\n",
    "    X = torch.cat([new_hidden_state, new_content_state, X], dim=2)\n",
    "    for i in range(num_steps):\n",
    "        # 开启预测模式,进行预测\n",
    "        Y, state = net.decoder(X, state, True)\n",
    "        X = Y.argmax(dim=2)\n",
    "        # 获取最大概率的index\n",
    "        pred = X.squeeze(dim=0).type(torch.int32).item()\n",
    "        # 如果index为eos,则停止预测\n",
    "        if pred == target_Vocab.word_to_index['<eos>']:\n",
    "            break\n",
    "        X = net.decoder.embedding(X).permute(1, 0, 2)\n",
    "        X = torch.cat([new_hidden_state, new_content_state, X], dim=2)\n",
    "        result.append(pred)\n",
    "    return ' '.join(target_Vocab.to_word(result))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predict(net, 'Can anybody stop them?', source_Vocab, target_Vocab, num_steps, device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
